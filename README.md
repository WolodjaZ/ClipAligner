# ClipAligner: Standing on the Shoulders of Giants to See and Read Further

PyTorch implementation and pretrained models for maybe future paper *ClipAligner: Standing on the Shoulders of Giants to See and Read Further*.

**ClipAligner** uses pretrained foundational models in image and text domains to align them in a common space. The alignment is done by training a alignment layers on top of foundational models on the aligned data image/captions pairs. The models are trained similar to the [CLIP](https://github.com/openai/CLIP) and [OpenCLIP](https://github.com/mlfoundations/open_clip).

## Pretrained models

TODO

## Installation

TODO

## Usage

### Prepare data

TODO

### Add foundationel models

#### Text

TODO

#### Image

TODO

### Train

TODO

### Evaluate

TODO

## Notebooks

TODO

## Contributing

TODO

## License

State the type of license the project is under (link to the LICENSE file).

## Acknowledgments

TODO

## References

TODO
